{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Los MiserAIbles\n",
    "Sorany Hincapie Salazar  \n",
    "Brayan Montoya Osorio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploración de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = 'data/challenge_data-18-ago.csv'\n",
    "\n",
    "df = pd.read_csv(path_to_data, sep = ';')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 3565, Number of columns: 3\n"
     ]
    }
   ],
   "source": [
    "rows, columns = df.shape\n",
    "print(f\"Number of rows: {rows}, Number of columns: {columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    " df = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracción de características con NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasos a seguir:\n",
    "1. Preprocesamiento del texto\n",
    "\n",
    "- Limpieza: eliminar caracteres especiales, números innecesarios, URLs, referencias\n",
    "- Normalización: convertir a minúsculas, manejar acentos y caracteres especiales\n",
    "- Tokenización: dividir el texto en palabras/tokens individuales\n",
    "- Eliminación de stopwords: quitar palabras comunes sin valor semántico (\"el\", \"la\", \"de\", \"and\", \"the\")\n",
    "- Stemming/Lemmatización: reducir palabras a su raíz o forma base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.  Extracción de características textuales\n",
    "Métodos tradicionales:\n",
    "\n",
    "- Bag of Words (BoW): frecuencia de palabras\n",
    "- TF-IDF: Term Frequency - Inverse Document Frequency\n",
    "- N-gramas: combinaciones de 2-3 palabras consecutivas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Entrenamiento modelos clasicos\n",
    "4. Reducción de dimensionalidad\n",
    "5. Re-entrenamiento\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo de test con los pasos anteriores:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Tokenización:  \n",
    "Dividir texto en palabras, oraciones o elementos pequeños. Ej: Convertir párrafo en lista de palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_example = df.iloc[0]['abstract']\n",
    "print(abstract_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenize_text(abstract_example)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Limpieza:\n",
    "Eliminar caracteres especiales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_basic_tokens(tokens):\n",
    "    return [token.lower() for token in tokens if token.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tokens = clean_basic_tokens(tokens)\n",
    "print(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Stemming:  \n",
    "Llevar palabras a su forma raíz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "def stem_words(words):\n",
    "    return [stemmer.stem(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_tokens = stem_words(cleaned_tokens)\n",
    "print(stemmed_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Eliminar StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens):\n",
    "    from nltk.corpus import stopwords\n",
    "    academic_stopwords = {\n",
    "        'abstract', 'paper', 'study', 'research', 'article', 'journal',\n",
    "        'analysis', 'method', 'approach', 'technique', 'result', 'conclusion',\n",
    "        'introduction', 'discussion', 'experimental', 'theoretical',\n",
    "        'also', 'however', 'therefore', 'furthermore', 'moreover'\n",
    "    }\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [token for token in tokens if token not in stop_words and token not in academic_stopwords]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tokens = remove_stopwords(stemmed_tokens)\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procesamiento de dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessNPL(text):\n",
    "    tokens = tokenize_text(text)\n",
    "    clean_tokens = clean_basic_tokens(tokens)\n",
    "    stemmed_tokens = stem_words(clean_tokens)\n",
    "    filtered_tokens = remove_stopwords(stemmed_tokens)\n",
    "    return filtered_tokens\n",
    "\n",
    "\n",
    "df['tokens_abstract'] = df['abstract'].apply(preprocessNPL)\n",
    "df['tokens_title'] = df['title'].apply(preprocessNPL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfstr=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfstr['tokens_abstract'] = df['tokens_abstract'].apply(lambda x: \" \".join(x))\n",
    "dfstr['tokens_title'] = df['tokens_title'].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfstr.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from deap import base, creator, tools, algorithms\n",
    "import random\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizador para tokens de títulos (cambia valores string a matriz sparse: númerica)\n",
    "tfidf_title = TfidfVectorizer(max_features=5000)#Ajustable\n",
    "X_title = tfidf_title.fit_transform(dfstr['tokens_title'])\n",
    "\n",
    "# vectorizador para tokens de abstracts\n",
    "tfidf_abstract = TfidfVectorizer(max_features=15000)\n",
    "X_abstract = tfidf_abstract.fit_transform(dfstr['tokens_abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_tokens = np.argsort(np.array(X_abstract.sum(axis=0)).ravel())[-8000:]\n",
    "Xcarac = X_abstract[:, top_tokens]\n",
    "feature_names = np.array(tfidf_abstract.get_feature_names_out())[top_tokens]#Asigna nombres a las columnas con las que se entrena el modelo LGBM\n",
    "                    #la posición del array indica el índice de Xcarac(subset de tokens más repetidos)\n",
    "y = df['group']  #etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = Xcarac.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(Xcarac, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness_function(individual):\n",
    "    selected_indices = [i for i, bit in enumerate(individual) if bit == 1]#Lista de 1 y 0s que indican token relevante a entrenar.\n",
    "    if len(selected_indices) == 0:\n",
    "        return 0.,  # evitar entrenamiento con 0 características\n",
    "\n",
    "    clf = lgb.LGBMClassifier(\n",
    "        n_estimators=50,\n",
    "        max_depth=5,\n",
    "        min_child_samples=5,\n",
    "        min_gain_to_split=0.0,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    #sparse matrix (n_muestras, n_features). Toma las características de acuerdo a los índices(col) indicados; conserva número de filas\n",
    "    X_train_sel = X_train[:, selected_indices]\n",
    "    X_test_sel  = X_test[:, selected_indices]\n",
    "\n",
    "    #LightGBM: acepta matrices sparse\n",
    "    clf.fit(X_train_sel, y_train, feature_name=[feature_names[i] for i in selected_indices])\n",
    "\n",
    "    acc = clf.score(X_test_sel, y_test)#accuracy\n",
    "    return acc,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración DEAP\n",
    "# Crear clase de fitness y individuo\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"attr_bool\", random.randint, 0, 1)\n",
    "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_bool, n=n_features)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "toolbox.register(\"evaluate\", fitness_function)\n",
    "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.05)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_size = 12\n",
    "n_gen = 6\n",
    "\n",
    "population = toolbox.population(n=pop_size)\n",
    "\n",
    "algorithms.eaSimple(\n",
    "    population,\n",
    "    toolbox,\n",
    "    cxpb=0.5,\n",
    "    mutpb=0.2,\n",
    "    ngen=n_gen,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "#Mejor individuo\n",
    "best_ind = tools.selBest(population, 1)[0]\n",
    "selected_features_final = [feature_names[i] for i, bit in enumerate(best_ind) if bit == 1]#tokens más significativos según el módelo\n",
    "\n",
    "print(\"Número de tokens seleccionados:\", len(selected_features_final))\n",
    "print(\"Tokens seleccionados:\", selected_features_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_features: tokens que DEAP seleccionó\n",
    "vocab = set(vectorizer.get_feature_names_out())\n",
    "valid_tokens = [t for t in selected_features if t in vocab]\n",
    "print(len(valid_tokens))\n",
    "print(len(selected_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_index = {t: i for i, t in enumerate(vectorizer.get_feature_names_out())}#Crea un diccionario con todos los tokens que el vectorizador(TF-IDF) identificó.\n",
    "selected_indices = [token_to_index[t] for t in valid_tokens]#Asigna un índice a cada token. \n",
    "\n",
    "X_final = X_abstract[:, selected_indices]\n",
    "print(\"Tamaño de la matriz final:\", X_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "original_columns = ['title', 'abstract']\n",
    "\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "X_original_sparse = encoder.fit_transform(dfstr[original_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "X_total = hstack([X_final, X_original_sparse])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dfstr['group'].str.split('|')\n",
    "mlb = MultiLabelBinarizer()\n",
    "Y = mlb.fit_transform(y)  # ahora cada columna es una categoría"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import f1_score, hamming_loss\n",
    "#Clasificación con tokens seleccionados y columnas title y abstract\n",
    "X_ = X_total\n",
    "y_ = Y\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_, y_, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "\n",
    "clf = OneVsRestClassifier(\n",
    "    lgb.LGBMClassifier(n_estimators=100, max_depth=7, n_jobs=-1)\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "y_predlg = clf.predict(X_test)\n",
    "\n",
    "# Métricas multilabel\n",
    "f1 = f1_score(y_test, y_pred, average='micro')\n",
    "hamming = hamming_loss(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_predlg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clasificador XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Separar las etiquetas por '|'\n",
    "y_multilabel = [y.split('|') for y in dfstr['group']]\n",
    "\n",
    "mlb = MultiLabelBinarizer()#transformar cada etiqueta en una columna 0/1\n",
    "Y = mlb.fit_transform(y_multilabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    Xcarac, Y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_xgb = xgb_multi = OneVsRestClassifier(xgb.XGBClassifier(objective='binary:logistic', n_estimators=10,\n",
    "                            seed=123))\n",
    "\n",
    "clf_xgb.fit(x_train, y_train)\n",
    "preds_xgb = clf_xgb.predict(x_test)\n",
    "\n",
    "print(classification_report(y_test, preds_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "#Probabilidades\n",
    "probs_xgb = clf_xgb.predict_proba(x_test)\n",
    "probs_lgb = clf.predict_proba(X_test)\n",
    "\n",
    "f1_xgb = f1_score(y_test, clf_xgb.predict(x_test), average='micro')\n",
    "f1_lgb = f1_score(y_test, clf.predict(X_test), average='micro')\n",
    "\n",
    "weights = np.array([f1_xgb, f1_lgb])\n",
    "weights = weights / weights.sum()\n",
    "\n",
    "# Votación ponderada\n",
    "combined_probs = probs_xgb * weights[0] + probs_lgb * weights[1]\n",
    "y_predcom = (combined_probs >= 0.7).astype(int)#umbral de clasificación según probabilidad combinada\n",
    "\n",
    "# Métricas multilabel\n",
    "f1_combined = f1_score(y_test, y_predcom, average='micro')\n",
    "print(\"F1 micro combinado:\", f1_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y_test, y_predcom, output_dict=True)\n",
    "df_report = pd.DataFrame(report).transpose()\n",
    "print(df_report)\n",
    "hamming = hamming_loss(y_test, y_predcom)\n",
    "print(\"-----------------------------\")\n",
    "print(\"Hamming loss:\", hamming)#proporción de etiquetas incorrectas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "med-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
